{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c307d0-a5bf-42da-8b8c-f8d420195ddf",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Forge Flask API - The Real Engine.\n",
    "Active Intelligence Enabled.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from gevent import monkey\n",
    "monkey.patch_all() # Patches everything\n",
    "\n",
    "import socket\n",
    "import requests\n",
    "from flask import Flask, jsonify, request, send_file\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, join_room\n",
    "from werkzeug.exceptions import HTTPException, RequestEntityTooLarge\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "# --- ACTIVE INTELLIGENCE IMPORTS ---\n",
    "from ai_bridge import BridgeValidationError, run_pipeline\n",
    "from compression import hyper_compress\n",
    "from decompress import decompress_blob\n",
    "from extract import extract_binary_data\n",
    "from job_updates import emit_job_update, set_emitter\n",
    "from map_and_scramble import geometric_map_and_scramble, geometric_unscramble_image\n",
    "from neuro_shatter import run_ingestion_convert, validate_and_clean, is_tabular_package\n",
    "from security import cryptographic_seal\n",
    "from storage import RedisJobStore, SqliteJobStore\n",
    "from stego_engine import embed_steganographic\n",
    "from unmask import unmask_alpha_layers\n",
    "from unlock import unlock_and_decrypt\n",
    "from unshuffle import reverse_geometric_scramble\n",
    "from verify import verify_and_restore\n",
    "\n",
    "BASE_DIR = Path(__file__).resolve().parent\n",
    "UPLOAD_DIR = BASE_DIR / \"uploads\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENGINE_MODE = os.environ.get(\"SNOWFLAKE_ENGINE_MODE\", \"engine\").lower()\n",
    "\n",
    "PHASES = [\n",
    "    {\n",
    "        \"id\": \"prepare\",\n",
    "        \"label\": \"Preparation & Profiling\",\n",
    "        \"description\": \"Ingest data, run YData profiling, and validate integrity.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"convert\",\n",
    "        \"label\": \"Neuroglyph / Shatter\",\n",
    "        \"description\": \"Apply intelligent serialization based on data profile.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"compress\",\n",
    "        \"label\": \"Hyper-Compression\",\n",
    "        \"description\": \"Zstandard Level 22 (Ultra).\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"map_and_scramble\",\n",
    "        \"label\": \"PassageMath Geometry\",\n",
    "        \"description\": \"Map payload into polyhedral geometry.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"stego_embed\",\n",
    "        \"label\": \"Stego Embed\",\n",
    "        \"description\": \"Embed payload into carrier image.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"seal\",\n",
    "        \"label\": \"Cryptographic Seal\",\n",
    "        \"description\": \"AES-256-GCM + KDF Hardening.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "GLOBAL_OPTIONS = {\n",
    "    \"highSecurity\": True,\n",
    "    \"passageMath\": True,\n",
    "    \"neuroShatter\": \"force\", # Default to ACTIVE\n",
    "}\n",
    "\n",
    "UPLOAD_REGISTRY: dict[str, dict[str, Any]] = {}\n",
    "OUTPUT_REGISTRY: list[dict[str, Any]] = []\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config[\"SECRET_KEY\"] = os.environ.get(\"SNOWFLAKE_SECRET\", \"forge-secret\")\n",
    "app.config[\"SOCKET_AUTH_TOKEN\"] = os.environ.get(\"FORGE_SOCKET_TOKEN\", app.config[\"SECRET_KEY\"])\n",
    "app.config[\"MAX_CONTENT_LENGTH\"] = 10 * 1024 * 1024 * 1024\n",
    "\n",
    "# 1. ALLOW ALL ORIGINS (Fixes your CORS issue permanently)\n",
    "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "socketio = SocketIO(\n",
    "    app,\n",
    "    cors_allowed_origins=\"*\",\n",
    "    async_mode=\"gevent\",  # Explicitly requesting High Performance mode\n",
    "    ping_interval=25,\n",
    "    ping_timeout=60,\n",
    ")\n",
    "set_emitter(lambda job_id, job_data: socketio.emit(\"job_update\", job_data, room=job_id))\n",
    "\n",
    "jobs: dict[str, dict[str, Any]] = {}\n",
    "jobs_lock = threading.Lock()\n",
    "redis_store = RedisJobStore(os.environ.get(\"REDIS_URL\"))\n",
    "persistent_store = SqliteJobStore(os.environ.get(\"SNOWFLAKE_DB_PATH\", str(BASE_DIR / \"data\" / \"snowflake.db\")))\n",
    "\n",
    "def _now() -> str:\n",
    "    return datetime.now().isoformat()\n",
    "\n",
    "def _error_response(message: str, status_code: int = 400) -> tuple[Any, int]:\n",
    "    return jsonify({\"error\": message}), status_code\n",
    "\n",
    "def _save_upload(file_storage) -> dict[str, Any]:\n",
    "    if not file_storage:\n",
    "        raise ValueError(\"Missing file upload\")\n",
    "    file_id = str(uuid.uuid4())\n",
    "    filename = secure_filename(file_storage.filename or \"upload.bin\")\n",
    "    path = UPLOAD_DIR / f\"{file_id}_{filename}\"\n",
    "    file_storage.save(path)\n",
    "    metadata = {\n",
    "        \"id\": file_id,\n",
    "        \"name\": filename,\n",
    "        \"path\": str(path),\n",
    "        \"size\": path.stat().st_size,\n",
    "        \"mime\": file_storage.mimetype or \"application/octet-stream\",\n",
    "    }\n",
    "    UPLOAD_REGISTRY[file_id] = metadata\n",
    "    return metadata\n",
    "\n",
    "def _get_job_from_store(job_id: str) -> dict[str, Any] | None:\n",
    "    if redis_store.enabled():\n",
    "        job = redis_store.get_job(job_id)\n",
    "        if job: return job\n",
    "    return persistent_store.get_job(job_id)\n",
    "\n",
    "def _get_job(job_id: str) -> dict[str, Any] | None:\n",
    "    with jobs_lock:\n",
    "        job = jobs.get(job_id)\n",
    "        if job: return job\n",
    "    job = _get_job_from_store(job_id)\n",
    "    if job:\n",
    "        with jobs_lock: jobs[job_id] = job\n",
    "    return job\n",
    "\n",
    "def _update_job(job_id: str, updates: dict[str, Any]) -> None:\n",
    "    with jobs_lock:\n",
    "        job = jobs.get(job_id)\n",
    "    if not job:\n",
    "        job = _get_job_from_store(job_id)\n",
    "    if not job: return\n",
    "    \n",
    "    previous_status = job.get(\"status\")\n",
    "    job.update(updates)\n",
    "    job[\"updatedAt\"] = _now()\n",
    "    \n",
    "    with jobs_lock: jobs[job_id] = job\n",
    "    redis_store.save_job(job_id, job)\n",
    "    persistent_store.upsert_job(job)\n",
    "\n",
    "    status = updates.get(\"status\")\n",
    "    if status and status != previous_status:\n",
    "        persistent_store.record_event(job_id, \"status_change\", {\"from\": previous_status, \"to\": status})\n",
    "        if status in {\"completed\", \"error\"}:\n",
    "            redis_store.archive_job(job_id)\n",
    "\n",
    "    payload = dict(job)\n",
    "    if job.get(\"geometricTelemetry\"):\n",
    "        payload[\"geometric_telemetry\"] = job[\"geometricTelemetry\"]\n",
    "    emit_job_update(job_id, payload)\n",
    "\n",
    "def _build_progress() -> dict[str, int]:\n",
    "    return {phase[\"id\"]: 0 for phase in PHASES}\n",
    "\n",
    "def _resolve_uploaded_file(file_id: str | None) -> str | None:\n",
    "    if not file_id: return None\n",
    "    metadata = UPLOAD_REGISTRY.get(file_id)\n",
    "    if not metadata: return None\n",
    "    return metadata[\"path\"]\n",
    "\n",
    "def _b64encode_bytes(payload: bytes | None) -> str | None:\n",
    "    if payload is None: return None\n",
    "    return base64.b64encode(payload).decode(\"utf-8\")\n",
    "\n",
    "def _read_upload_bytes(field: str = \"file\") -> tuple[bytes, str]:\n",
    "    file_storage = request.files.get(field)\n",
    "    if not file_storage: raise ValueError(f\"Missing file upload: {field}\")\n",
    "    return file_storage.read(), file_storage.filename or field\n",
    "\n",
    "def _form_int(key: str, default: int) -> int:\n",
    "    raw = request.form.get(key)\n",
    "    return int(raw) if raw else default\n",
    "\n",
    "def _form_bool(key: str, default: bool) -> bool:\n",
    "    raw = request.form.get(key)\n",
    "    return raw.lower() in {\"1\", \"true\", \"yes\", \"on\"} if raw is not None else default\n",
    "\n",
    "def _resolve_job_id() -> str | None:\n",
    "    if request.is_json:\n",
    "        payload = request.get_json(silent=True) or {}\n",
    "        if payload.get(\"jobId\"): return payload.get(\"jobId\")\n",
    "    return request.form.get(\"jobId\") or request.args.get(\"jobId\")\n",
    "\n",
    "def _emit_restoration_metrics(job_id: str | None, result: dict[str, Any]) -> None:\n",
    "    if job_id and result.get(\"rs_healing_triggered\"):\n",
    "        emit_job_update(job_id, {\"jobId\": job_id, \"restoration_metrics\": result.get(\"restoration_metrics\")})\n",
    "\n",
    "def _build_verify_response(result: dict[str, Any]) -> Any:\n",
    "    return jsonify({\n",
    "        \"verified_data\": result.get(\"verified_data\"),\n",
    "        \"restoration\": result.get(\"restoration\"),\n",
    "        \"healed_payload_b64\": _b64encode_bytes(result.get(\"healed_payload\")),\n",
    "        \"restoration_metrics\": result.get(\"restoration_metrics\"),\n",
    "    })\n",
    "\n",
    "# --- ROUTES ---\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def health() -> Any:\n",
    "    return jsonify({\"status\": \"online\", \"service\": \"Forge Engine\", \"version\": \"2.0-Active\", \"timestamp\": _now()})\n",
    "\n",
    "@app.route(\"/api/options\", methods=[\"GET\"])\n",
    "def get_options() -> Any:\n",
    "    return jsonify({\"phases\": PHASES, \"globalOptions\": GLOBAL_OPTIONS})\n",
    "\n",
    "@app.route(\"/api/uploads\", methods=[\"POST\"])\n",
    "def upload_file() -> Any:\n",
    "    file = request.files.get(\"file\")\n",
    "    if not file: return _error_response(\"Missing file\", 400)\n",
    "    try:\n",
    "        metadata = _save_upload(file)\n",
    "        return jsonify(metadata)\n",
    "    except Exception:\n",
    "        app.logger.exception(\"Upload failed\")\n",
    "        return _error_response(\"Upload failed\", 500)\n",
    "\n",
    "@app.route(\"/api/encapsulate\", methods=[\"POST\"])\n",
    "def encapsulate() -> Any:\n",
    "    try:\n",
    "        if request.is_json:\n",
    "            payload = request.get_json(silent=True) or {}\n",
    "            options = payload.get(\"options\", {})\n",
    "            target_ids = payload.get(\"targetFileIds\", [])\n",
    "            carrier_id = payload.get(\"carrierFileId\")\n",
    "            target_paths = [p for fid in target_ids if (p := _resolve_uploaded_file(fid))]\n",
    "            carrier_path = _resolve_uploaded_file(carrier_id)\n",
    "        else:\n",
    "            options = json.loads(request.form.get(\"options\", \"{}\"))\n",
    "            target_files = request.files.getlist(\"target_files\")\n",
    "            carrier_file = request.files.get(\"carrier_image\")\n",
    "            target_paths = [_save_upload(f)[\"path\"] for f in target_files]\n",
    "            carrier_path = _save_upload(carrier_file)[\"path\"] if carrier_file else None\n",
    "\n",
    "        if not target_paths or not carrier_path:\n",
    "            return _error_response(\"Missing target files or carrier image\", 400)\n",
    "\n",
    "        job_id = str(uuid.uuid4())\n",
    "        job = {\n",
    "            \"jobId\": job_id,\n",
    "            \"type\": \"encapsulation\",\n",
    "            \"status\": \"queued\",\n",
    "            \"phase\": 0,\n",
    "            \"phaseId\": None,\n",
    "            \"progress\": _build_progress(),\n",
    "            \"metrics\": {\n",
    "                \"compressionRatio\": 0,\n",
    "                \"originalSize\": sum(Path(p).stat().st_size for p in target_paths),\n",
    "                \"compressedSize\": 0,\n",
    "            },\n",
    "            \"targetPaths\": target_paths,\n",
    "            \"carrierPath\": carrier_path,\n",
    "            \"options\": {**GLOBAL_OPTIONS, **options},\n",
    "            \"createdAt\": _now(),\n",
    "            \"updatedAt\": _now(),\n",
    "        }\n",
    "\n",
    "        with jobs_lock: jobs[job_id] = job\n",
    "        redis_store.save_job(job_id, job)\n",
    "        persistent_store.upsert_job(job)\n",
    "        \n",
    "        # Start the REAL pipeline thread\n",
    "        thread = threading.Thread(target=_process_encapsulation, args=(job_id,))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "\n",
    "        return jsonify({\"jobId\": job_id, \"status\": \"queued\"}), 200\n",
    "    except Exception:\n",
    "        app.logger.exception(\"Encapsulation request failed\")\n",
    "        return _error_response(\"Failed to start encapsulation\", 500)\n",
    "\n",
    "@app.route(\"/api/job/<job_id>\", methods=[\"GET\"])\n",
    "def get_job_status(job_id: str) -> Any:\n",
    "    job = _get_job(job_id)\n",
    "    if not job: return _error_response(\"Job not found\", 404)\n",
    "    return jsonify({k: v for k, v in job.items() if k not in [\"targetPaths\", \"carrierPath\"]})\n",
    "\n",
    "@app.route(\"/api/jobs\", methods=[\"GET\"])\n",
    "def list_jobs() -> Any:\n",
    "    limit = int(request.args.get(\"limit\", \"100\"))\n",
    "    status = request.args.get(\"status\")\n",
    "    jobs_list = persistent_store.list_jobs(limit=limit, status=status)\n",
    "    return jsonify({\"jobs\": [j for j in jobs_list if j.get(\"status\") != \"error\"]})\n",
    "\n",
    "@app.route(\"/api/download/<job_id>\", methods=[\"GET\"])\n",
    "def download_result(job_id: str) -> Any:\n",
    "    job = _get_job(job_id)\n",
    "    if not job or job.get(\"status\") != \"completed\": return _error_response(\"Job not ready\", 400)\n",
    "    output_path = job.get(\"outputPath\")\n",
    "    if not output_path or not Path(output_path).exists(): return _error_response(\"File missing\", 404)\n",
    "    return send_file(output_path, mimetype=\"image/png\", as_attachment=True, download_name=f\"forge_{job_id}.png\")\n",
    "\n",
    "# --- ENGINE LOGIC (THE CONDUCTOR) ---\n",
    "\n",
    "def _run_engine_pipeline(job_id: str, job: dict[str, Any], output_path: Path) -> dict[str, Any]:\n",
    "    options = job.get(\"options\", {})\n",
    "    payload_path = Path(job[\"targetPaths\"][0])\n",
    "    carrier_path = Path(job[\"carrierPath\"])\n",
    "    progress = job.get(\"progress\", _build_progress())\n",
    "    context: dict[str, Any] = {}\n",
    "\n",
    "    def mark_progress(phase_id: str, percent: int = 100) -> None:\n",
    "        progress[phase_id] = percent\n",
    "        _update_job(job_id, {\"progress\": progress})\n",
    "\n",
    "    # --- PHASE 1: PREPARE & PROFILE ---\n",
    "    _update_job(job_id, {\"phase\": 1, \"phaseId\": \"prepare\"})\n",
    "    payload_bytes = payload_path.read_bytes()\n",
    "    package = validate_and_clean({\"file\": payload_bytes, \"name\": payload_path.name})\n",
    "    context[\"package\"] = package\n",
    "    mark_progress(\"prepare\")\n",
    "\n",
    "    # --- PHASE 2: INTELLIGENT CONVERSION ---\n",
    "    _update_job(job_id, {\"phase\": 2, \"phaseId\": \"convert\"})\n",
    "    \n",
    "    # Active Intelligence: Check if tabular and FORCE profile\n",
    "    if is_tabular_package(package):\n",
    "        print(f\"[{job_id}] ðŸ§  Intelligent Ingestion: Tabular Data Detected.\")\n",
    "        # Override option to ensure YData runs\n",
    "        options[\"neuroShatter\"] = \"force\"\n",
    "        options[\"neuro_shatter\"] = \"force\"\n",
    "    else:\n",
    "        print(f\"[{job_id}] ðŸ“· Intelligent Ingestion: Binary/Image Data Detected.\")\n",
    "\n",
    "    # Run Ingestion\n",
    "    conversion_result = run_ingestion_convert(package, options)\n",
    "    context.update(conversion_result)\n",
    "    \n",
    "    # Log findings\n",
    "    if \"neuro_shatter_report\" in conversion_result:\n",
    "        report = conversion_result[\"neuro_shatter_report\"]\n",
    "        cols = len(report.get(\"variables\", {}))\n",
    "        print(f\"[{job_id}] ðŸ“Š YData Profile Complete: {cols} variables analyzed.\")\n",
    "    \n",
    "    mark_progress(\"convert\")\n",
    "\n",
    "    # --- PHASE 3: HYPER COMPRESSION ---\n",
    "    _update_job(job_id, {\"phase\": 3, \"phaseId\": \"compress\"})\n",
    "    # Only compress if NeuroShatter didn't already do the job\n",
    "    if context.get(\"compressed_blob\") is None:\n",
    "        context.update(hyper_compress(context[\"patternized_blob\"], level=options.get(\"zstdLevel\", 22)))\n",
    "    mark_progress(\"compress\")\n",
    "\n",
    "    # --- PHASE 4: PASSAGEMATH GEOMETRY ---\n",
    "    _update_job(job_id, {\"phase\": 4, \"phaseId\": \"map_and_scramble\"})\n",
    "    polytope_type = options.get(\"polytopeType\", \"cube\")\n",
    "    carrier_bytes = carrier_path.read_bytes()\n",
    "    \n",
    "    # \"Try-Hard\" Logic for PassageMath\n",
    "    try:\n",
    "        print(f\"[{job_id}] ðŸ“ Constructing {polytope_type} Polytope via PassageMath...\")\n",
    "        result = geometric_map_and_scramble(\n",
    "            context[\"compressed_blob\"],\n",
    "            carrier_bytes,\n",
    "            polytope_type=polytope_type,\n",
    "            backend=\"passagemath\" # FORCE IT\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[{job_id}] âš ï¸ PassageMath Backend Unstable: {e}\")\n",
    "        print(f\"[{job_id}] ðŸ”„ Rerouting to Latte Backend.\")\n",
    "        result = geometric_map_and_scramble(\n",
    "            context[\"compressed_blob\"],\n",
    "            carrier_bytes,\n",
    "            polytope_type=polytope_type,\n",
    "            backend=\"latte\" # Fallback\n",
    "        )\n",
    "\n",
    "    context.update(result)\n",
    "    _update_job(job_id, {\n",
    "        \"geometricTelemetry\": {\n",
    "            \"fVector\": result.get(\"f_vector\"),\n",
    "            \"polytopeType\": polytope_type,\n",
    "        },\n",
    "        \"geometricKey\": result.get(\"permutation_key\"),\n",
    "    })\n",
    "    mark_progress(\"map_and_scramble\")\n",
    "\n",
    "    # --- PHASE 5: STEGO EMBED ---\n",
    "    _update_job(job_id, {\"phase\": 5, \"phaseId\": \"stego_embed\"})\n",
    "    context.update(embed_steganographic(\n",
    "        context[\"compressed_blob\"],\n",
    "        context[\"scrambled_carrier\"],\n",
    "        password=options.get(\"passphrase\") or \"supersecret\",\n",
    "        layers=options.get(\"stegoLayers\", 2),\n",
    "        dynamic=True\n",
    "    ))\n",
    "    context[\"unscrambled_image\"] = geometric_unscramble_image(\n",
    "        context[\"embedded_image\"],\n",
    "        context[\"permutation_key\"],\n",
    "    )\n",
    "    mark_progress(\"stego_embed\")\n",
    "\n",
    "    # --- PHASE 6: CRYPTOGRAPHIC SEAL ---\n",
    "    _update_job(job_id, {\"phase\": 6, \"phaseId\": \"seal\"})\n",
    "    context.update(cryptographic_seal(\n",
    "        context[\"unscrambled_image\"],\n",
    "        password=options.get(\"passphrase\"),\n",
    "        kdf_iterations=100_000, # Luxury Setting: 100k\n",
    "        user_data={\"permutation_key\": context.get(\"permutation_key\")}\n",
    "    ))\n",
    "    mark_progress(\"seal\")\n",
    "\n",
    "    # Finalize\n",
    "    sealed_image = context.get(\"sealed_image\")\n",
    "    output_path.write_bytes(sealed_image)\n",
    "    \n",
    "    metrics = job.get(\"metrics\", {}).copy()\n",
    "    metrics[\"originalSize\"] = job[\"metrics\"][\"originalSize\"]\n",
    "    metrics[\"compressedSize\"] = len(context.get(\"compressed_blob\") or b\"\")\n",
    "    metrics[\"compressionRatio\"] = context.get(\"compression_ratio\", 1.0)\n",
    "    \n",
    "    return {\n",
    "        \"output_path\": output_path,\n",
    "        \"metrics\": metrics,\n",
    "        \"geometric_key\": context.get(\"permutation_key\"),\n",
    "        \"payload_size\": metrics[\"compressedSize\"],\n",
    "    }\n",
    "\n",
    "def _process_encapsulation(job_id: str) -> None:\n",
    "    job = _get_job(job_id)\n",
    "    if not job: return\n",
    "\n",
    "    _update_job(job_id, {\"status\": \"processing\", \"phase\": 1, \"phaseId\": PHASES[0][\"id\"]})\n",
    "    output_path = OUTPUT_DIR / f\"{job_id}.png\"\n",
    "\n",
    "    try:\n",
    "        # ALWAYS RUN THE ENGINE. NO MOCKS.\n",
    "        result = _run_engine_pipeline(job_id, job, output_path)\n",
    "\n",
    "        OUTPUT_REGISTRY.append({\n",
    "            \"jobId\": job_id,\n",
    "            \"createdAt\": job.get(\"createdAt\"),\n",
    "            \"payloadSize\": result.get(\"payload_size\", 0),\n",
    "            \"outputPath\": str(result[\"output_path\"]),\n",
    "        })\n",
    "\n",
    "        _update_job(job_id, {\n",
    "            \"status\": \"completed\",\n",
    "            \"metrics\": result[\"metrics\"],\n",
    "            \"outputPath\": str(result[\"output_path\"]),\n",
    "            \"geometricKey\": result.get(\"geometric_key\"),\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        app.logger.exception(f\"Engine Failure {job_id}\")\n",
    "        _update_job(job_id, {\"status\": \"error\", \"error\": str(exc)})\n",
    "\n",
    "# --- HEALTH ---\n",
    "@app.route(\"/api/health/ai\", methods=[\"GET\"])\n",
    "def ai_health() -> Any:\n",
    "    ollama_url = os.environ.get(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "    ollama_health = {\"status\": \"unknown\", \"url\": ollama_url}\n",
    "    try:\n",
    "        # Give Ollama 10 seconds to wake up (User Preference)\n",
    "        response = requests.get(f\"{ollama_url}/api/version\", timeout=10)\n",
    "        if response.ok:\n",
    "            ollama_health.update({\"status\": \"healthy\", \"version\": response.json().get(\"version\")})\n",
    "        else:\n",
    "            ollama_health.update({\"status\": \"unhealthy\", \"error\": f\"HTTP {response.status_code}\"})\n",
    "    except Exception as e:\n",
    "        ollama_health.update({\"status\": \"unhealthy\", \"error\": str(e)})\n",
    "    \n",
    "    return jsonify({\"status\": \"ok\" if ollama_health[\"status\"]==\"healthy\" else \"degraded\", \"providers\": {\"ollama\": ollama_health}})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, host=\"0.0.0.0\", port=5000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a871522-5dce-4c98-92fc-fb6509586e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
